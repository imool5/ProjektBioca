\tableofcontents
\chapter{Wprowadzenie}

\section{Sztuczna Sieæ neuronowa - co to w³aœciwie jest?}

Definicj¹ sztucznej sieci neuronowej jest zbiór prostych jednostek obliczeniowych przetwarzaj¹cych dane,
komunikuj¹cych siê ze sob¹ i pracuj¹cych równolegle.

\section{Typy Sieci Neuronowych}

Wyró¿niamy 3 typy sieci neuronowych
\begin{itemize}
\item Sieci Jednokierunkowe
\item Sieci Rekurencyjne
\item Samoorganizuj¹ce siê mapy
\end{itemize}

\chapter{Sztuczne Sieci Neuronowe}

\section{Metoda regu³y delta}
Regu³a Delta zosta³a opracowana przez Widrowa i Hoffa, znalaz³a ona zastosowanie do uczenia elementów liniowych i nieliniowych.
Regu³a delta jest regu³¹ uczenia z nauczycielem. Polega ona na tym, ¿e ka¿dy neuron po otrzymaniu na swoich wejœciach okreœlone sygna³y (z wejœæ sieci albo od innych neuronów, stanowi¹cych wczeœniejsze piêtra przetwarzania informacji) wyznacza swój sygna³ wyjœciowy wykorzystuj¹c posiadan¹ wiedzê w postaci wczeœniej ustalonych wartoœci wspó³czynników wzmocnienia (wag) wszystkich wejœæ oraz (ewentualnie) progu. Sposoby wyznaczania przez neurony wartoœci sygna³ów wyjœciowych na podstawie sygna³ów wejœciowych omówione zosta³y dok³adniej w poprzednim rozdziale. Wartoœæ sygna³u wyjœciowego, wyznaczonego przez neuron na danym kroku procesu uczenia porównywana jest z odpowiedzi¹ wzorcow¹ podan¹ przez nauczyciela w ci¹gu ucz¹cym. Jeœli wystêpuje rozbie¿noœæ - neuron wyznacza ró¿nicê pomiêdzy swoim sygna³em wyjœciowym a t¹ wartoœci¹ sygna³u, która by³a by - wed³ug nauczyciela prawid³owa. Ta ró¿nica oznaczana jest zwykle symbolem greckiej litery delta i st¹d nazwa opisywanej metody. 

Sygna³ b³êdu wykorzystywany jest przez neuron do korygowania swoich wspó³czynników wagowych:
\begin{itemize}
\item wagi zmieniane s¹ tym silniej, im wiêkszy jest b³¹d
\item wagi zwi¹zane z tymi wejœciami, na których wystêpowa³y du¿e wartoœci sygna³ów wejœciowych zmieniane s¹ bardziej ni¿ wagi wejœæ, na których sygna³ wejœciowy by³ niewielki

\end{itemize}

Znaj¹c b³¹d pope³niony przez neuron oraz jego wagi wejsciowe mozemy latwo przewidziec jak beda siê zmieniac jego wagi.

\includegraphics[scale=0.7]{rysunki/Deltawzor.png} 

\section{Metoda wstecznej propagacji b³êdów}

Przez wiele lat nie znaleziono skutecznej metody uczenia sieci wielowarstwowych, dopiero w latach 80-tych zapropownowany zosta³ algorytm wstecznej propagacji b³êdów polegaj¹cy na tym, ¿e maj¹c wyznaczony b³¹d $\delta_m^{(j)}$ wystêpuj¹cy podczas realizacji j-tego kroku procesu uczenia w neuronie o numerze \textit{m} mo¿na podawaæ ten b³¹d wstecz do wszystkich tych neuronów, których sygna³y stanowi³y wejœcia dla m-tego neuronu.

Uczenie odbywa siê przez minimalizacjê odpowiednio zdefiniowanej funkcji celu Q(W), przy czym wektor W reprezentuje wagi sieci poddawane optymalizacji. Najprostsza funkcja celu ma postaæ b³êdu œredniokwadratowego. Zastosowanie ró¿niczkowalnej funkcji aktywacji umo¿liwia minimalizacjê funkcji celu metodami gradientowymi.

\section{Sieæ Hopfielda}
\begin{verbatim}
http://th-www.if.uj.edu.pl/~erichter/dydaktyka/Dydaktyka2012/SieciNN-2012/NN-wyklad-6-2012.pdf
\end{verbatim}


Sieæ Hopfielda jest najbardziej znan¹ sieci¹, w której kierunek przep³ywu sygna³ów jest odwrócony, posiada sprzê¿enia zwrotne typu ka¿dy z ka¿dym, jest prostym przyk³adem sieci rekurencyjnej i czêsto jest nazywana autoasocjatorem, a w ramach tego sprzê¿enia ka¿dy neuron jest po³¹czony z jednym z wejœæ oraz z w³asnym wyjœciem.
\includegraphics[scale=0.7]{rysunki/Hopfield1.PNG} 
\subsection{Metody uczenia sieci}
W sieciach hopfielda wykorzystujemy uczenie oparte na pseudoinwersji macierzy. Tak dobieramy wagi, aby uzyskaæ na wyjœciu takie same wzorce jakie podajemy na wejœciu.
\begin{equation}
\left. WX=
\right. X
\end{equation}
gdzie \textbf{W} to macierz wag o wymiarze n x n, a \textbf{X} to macierz wzorców o wymiarze n x p z³o¿on¹ z p wektorów ucz¹cych.
\section{Sieci Kohonena}







\begin{flushleft}
Tekst wyrównany do lewej.
\end{flushleft}
\begin{center}
Tekst wyœrodkowany.
\end{center}
\begin{flushright}
Tekst wyrównany do prawej.
\end{flushright}
\section{Podrozdzia³ 1}
\section{Podrozdzia³ 2}

\begin{itemize}
\item[-] \textbf{Pogrubiony tekst.}
\item \textit{Tekst pisany kursyw¹.}
\end{itemize}
\begin{enumerate}
\item punkt pierwszy
\item punkt drugi
\end{enumerate}
\subsection{Znaki Specjalne}
Hasztag \# \\
Backslash $\backslash$
 Dolar $\$$

 \begin{table} [h] 
 \begin{tabular}{|l|c|p{7cm}|} 
 \hline 
 kom 11 & kom 12  \\ 
 \hline 
 \hline
\multicolumn{2}{|c|}{kom 22 i 23}  \\ 
\hline
\multicolumn{2}{|l|}{kom 31 i kom 32}   \\ 
\hline 
 kom 41 & kom 42  \\ 
 \hline 
 \end{tabular} 
 \centering 
 \caption{tabela 1}\label{tab_1} 
 
 \end{table}

\ odwo³anie do tabeli (tab \ref{tab_1}):
 \begin{verbatim}
 
\begin{table} [h] 
 \begin{tabular}{|l|c|p{7cm}|} 
 \hline 
 kom 11 & kom 12  \\ 
 \hline 
 \hline
\multicolumn{2}{|c|}{kom 22 i 23}  \\ 
\hline
\multicolumn{2}{|l|}{kom 31 i kom 32}   \\ 
\hline 
 kom 41 & kom 42  \\ 
 \hline 
 \end{tabular} 
 \centering 
 \caption{tabela 1}\label{tab_1} 
 
\end{verbatim}

\begin{equation}\label{moje_równanie}
\left(\prod_{i=\widetilde{j}}^{\infty}[\log(i^{\xi})]^{M}\le0 \leftrightarrow \sum_{i=\widetilde{j}}^{\infty}\sqrt{{i}\over{\widetilde{j}}} >
\sqrt[p]{\widetilde{j}}\right )\Rightarrow \textrm{nic nie}\ wynika
\end{equation}
\newline
\ Odwo³anie do równania  (\ref{moje_równanie}):
\begin{verbatim}
\begin{equation}\label{moje_równanie}
\left(\prod_{i=\widetilde{j}}^{\infty}[\log(i^{\xi})]^{M}\le0
 \leftrightarrow
  \sum_{i=\widetilde{j}}^{\infty}\sqrt{{i}\over{\widetilde{j}}} >
\sqrt[p]{\widetilde{j}}\right )\Rightarrow \textrm{nic nie}\ wynika
\end{equation}
\end{verbatim}



\begin{figure}
\raggedright
  
\caption{ dwa rysunki jeden nad drugim}\label{rysunek_1}

\end{figure}

\ Odwo³anie do rysunku (rys \ref{rysunek_1}):
\begin{verbatim}
\begin{figure}
\raggedright
\includegraphics[scale=0.35]{rysunek1.jpg}
\includegraphics[scale=0.35]{rysunek1.jpg}  
\caption{ dwa rysunki jeden nad drugim}\label{rysunek_1}

\end{figure}
\end{verbatim}

\chapter{Bibliografia}
\begin{verbatim}
http://www.dbc.wroc.pl/Content/1908/Rusiecki_Algorytmy_PhD.pdf

https://platforma.polsl.pl/rib/pluginfile.php/2498/mod_resource/content/2/Laboratorium/04_BProp.pdf

http://www.neurosoft.edu.pl/media/pdf/tkwater/sztuczna_inteligencja/2_alg_ucz_ssn.pdf
\end{verbatim}



 \end{document}